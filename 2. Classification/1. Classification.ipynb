{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikaiton\n",
    "**Teori** - [**Notion Klassifikation**](https://www.notion.so/mercantec/Machine-Learning-e89a2baf0d414172b13d07465366482e?pvs=4#1a2aef001e294c9bb2354f79b3db80d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Billedklassifikation\n",
    "Denne vejledning viser, hvordan man klassificerer billeder af blomster ved hjælp af en `tf.keras.Sequential` model og indlæser data ved hjælp af `tf.keras.utils.image_dataset_from_directory`. Den demonstrerer følgende koncepter:\n",
    "\n",
    "- Effektiv indlæsning af et dataset fra disken.\n",
    "- Identifikation af overfitting og anvendelse af teknikker til at mindske det, herunder dataaugmentering og dropout.\n",
    "\n",
    "Denne vejledning følger en grundlæggende maskinlæringsarbejdsgang:\n",
    "\n",
    "1. **Undersøg og forstå data**\n",
    "2. **Byg en input-pipelining**\n",
    "3. **Opret modellen**\n",
    "4. **Træn modellen**\n",
    "5. **Test modellen**\n",
    "6. **Forbedr modellen og gentag processen**\n",
    "\n",
    "Derudover demonstrerer notebooken, hvordan man konverterer en [gemt model](../../../guide/saved_model.ipynb) til en [TensorFlow Lite](https://www.tensorflow.org/lite/) model til maskinlæring på mobile, integrerede og IoT-enheder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opsætning\n",
    "\n",
    "Importer TensorFlow og andre nødvendige biblioteker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hent og udforsk datasættet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denne vejledning bruger et datasæt med cirka 3.700 fotos af blomster. Datasættet indeholder fem undermapper, en pr. klasse:\n",
    "```\n",
    "flower_photo/\n",
    "  daisy/\n",
    "  dandelion/\n",
    "  roses/\n",
    "  sunflowers/\n",
    "  tulips/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
    "data_dir = pathlib.Path(data_dir).with_suffix('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efter download bør du nu have en kopi af datasættet tilgængelig. Der er i alt 3.670 billeder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er nogle roser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roses = list(data_dir.glob('roses/*'))\n",
    "PIL.Image.open(str(roses[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(roses[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og nogle tulipaner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tulips = list(data_dir.glob('tulips/*'))\n",
    "PIL.Image.open(str(tulips[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(tulips[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indlæs data ved hjælp af et Keras-værktøj\n",
    "\n",
    "Dernæst kan du indlæse disse billeder fra disken ved hjælp af den nyttige `tf.keras.utils.image_dataset_from_directory`-funktion. Dette vil tage dig fra en mappe med billeder på disken til en `tf.data.Dataset` med blot et par linjer kode. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opret en datasæt\n",
    "Definér nogle parametre for indlæseren:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det er god praksis at bruge en valideringssplit, når du udvikler din model. Brug 80% af billederne til træning og 20% til validering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du kan finde klassenavne i `class_names` attributten på disse datasæt. Disse svarer til mappenavne i alfabetisk rækkefølge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser dataen\n",
    "\n",
    "Her er de første ni billeder fra træningsdatasættet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senere i denne vejledning vil du overføre disse datasæt til Keras `Model.fit`-metoden til træning. Hvis du foretrækker det, kan du også manuelt iterere over datasættet og hente batcher af billeder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image_batch` er en tensor med formen `(32, 180, 180, 3)`. Dette er en batch med 32 billeder med formen `180x180x3` (den sidste dimension refererer til farvekanalerne RGB). `label_batch` er en tensor med formen `(32,)`, og det er de tilhørende etiketter til de 32 billeder.\n",
    "\n",
    "Du kan bruge `.numpy()` på `image_batch` og `label_batch` tensorerne for at konvertere dem til en `numpy.ndarray`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konfigurer datasættet for bedre ydeevne\n",
    "\n",
    "Sørg for at bruge bufferlagring (buffered prefetching), så du kan hente data fra disken uden at I/O bliver blokerende. Her er to vigtige metoder, du bør bruge, når du indlæser data:\n",
    "\n",
    "- `Dataset.cache` gemmer billederne i hukommelsen efter de er blevet indlæst fra disken under den første epoke. Dette vil sikre, at datasættet ikke bliver en flaskehals under træningen af din model. Hvis dit datasæt er for stort til at passe i hukommelsen, kan du også bruge denne metode til at oprette en ydeevnedrevet cache på disken.\n",
    "- `Dataset.prefetch` overlapper dataforarbejdning og modelkørsel under træning.\n",
    "\n",
    "Interesserede læsere kan lære mere om begge metoder, samt hvordan man gemmer data på disken, i afsnittet om *Bufferlagring (Prefetching)* i [Bedre ydeevne med tf.data API](../../guide/data_performance.ipynb) vejledningen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardiser dataen\n",
    "RGB-kanalværdierne er i området `[0, 255]`. Dette er ikke ideelt for et neuralt netværk; generelt bør du stræbe efter at gøre dine inputværdier små.\n",
    "\n",
    "Her vil du standardisere værdierne, så de er i området `[0, 1]` ved at bruge `tf.keras.layers.Rescaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der er to måder at bruge denne lag på. Du kan anvende det på datasættet ved at kalde `Dataset.map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eller du kan inkludere laget i din modeldefinition, hvilket kan forenkle implementeringen. Brug den anden tilgang her.\n",
    "Bemærk: Tidligere ændrede du størrelsen på billeder ved at bruge argumentet `image_size` i `tf.keras.utils.image_dataset_from_directory`. Hvis du også vil inkludere ændringen af størrelsen i din model, kan du bruge laget `tf.keras.layers.Resizing`.\n",
    "## En grundlæggende Keras-model\n",
    "\n",
    "### Opret modellen\n",
    "\n",
    "Keras [Sequential](https://www.tensorflow.org/guide/keras/sequential_model)-modellen består af tre konvolutionsblokke (`tf.keras.layers.Conv2D`) med en maksimal poolingslag (`tf.keras.layers.MaxPooling2D`) i hver af dem. Der er en fuldt forbundet lag (`tf.keras.layers.Dense`) med 128 enheder oven på det, som aktiveres af en ReLU-aktiveringsfunktion (`'relu'`). Denne model er ikke justeret for høj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompiler modellen\n",
    "\n",
    "Til denne vejledning vælg `tf.keras.optimizers.Adam`-optimeringsmetoden og `tf.keras.losses.SparseCategoricalCrossentropy` som tabfunktion. For at se trænings- og valideringsnøjagtighed for hver træningsepoke, overfør `metrics`-argumentet til `Model.compile`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeloversigt\n",
    "\n",
    "Se alle lagene i netværket ved hjælp af Keras `Model.summary`-metoden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Træn modellen\n",
    "\n",
    "Træn modellen i 10 epoker med Keras `Model.fit`-metoden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisér træningsresultater\n",
    "Opret grafer for tab og nøjagtighed på trænings- og valideringssættene:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graferne viser, at træningsnøjagtighed og valideringsnøjagtighed afviger betydeligt, og modellen har kun opnået omkring 60% nøjagtighed på valideringssættet.\n",
    "\n",
    "De følgende sektioner i vejledningen viser, hvordan du kan inspicere, hvad der gik galt, og forsøge at forbedre den samlede ydeevne af modellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "På graferne ovenfor stiger træningsnøjagtigheden lineært over tid, mens valideringsnøjagtigheden stagnerer omkring 60% i træningsprocessen. Derudover er forskellen i nøjagtighed mellem trænings- og valideringsnøjagtighed mærkbar - et tegn på [overfitting](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit).\n",
    "\n",
    "Når der er et lille antal træningseksempler, lærer modellen sommetider fra støj eller uønskede detaljer i træningseksemplerne i en sådan grad, at det negativt påvirker modellens ydeevne på nye eksempler. Dette fænomen kaldes overfitting. Det betyder, at modellen vil have svært ved at generalisere på et nyt datasæt.\n",
    "\n",
    "Der er flere måder at bekæmpe overfitting i træningsprocessen. I denne vejledning vil du bruge *dataaugmentering* og tilføje *dropout* til din model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataaugmentering\n",
    "Overfitting opstår normalt, når der er et lille antal træningseksempler. [Dataaugmentering](./data_augmentation.ipynb) anvender tilgangen med at generere yderligere træningsdata fra dine eksisterende eksempler ved at udvide dem med tilfældige transformationer, der producerer troværdigt udseende billeder. Dette hjælper med at udsætte modellen for flere aspekter af dataen og generalisere bedre.\n",
    "\n",
    "Du vil implementere dataaugmentering ved hjælp af følgende Keras-forbehandlingslag: `tf.keras.layers.RandomFlip`, `tf.keras.layers.RandomRotation` og `tf.keras.layers.RandomZoom`. Disse kan inkluderes i din model som andre lag og køre på GPU'en.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.RandomFlip(\"horizontal\",\n",
    "                      input_shape=(img_height,\n",
    "                                  img_width,\n",
    "                                  3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisér nogle augmenterede eksempler ved at anvende dataaugmentering til det samme billede flere gange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du vil tilføje dataaugmentering til din model inden træning i næste trin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "En anden teknik til at reducere overfitting er at introducere [dropout](https://developers.google.com/machine-learning/glossary#dropout_regularization){:.external}-regulering til netværket.\n",
    "\n",
    "Når du anvender dropout på et lag, udelukker det tilfældigt (ved at indstille aktiveringen til nul) et antal outputenheder fra laget under træningsprocessen. Dropout tager et brøkdeligt tal som inputværdi, i form som f.eks. 0,1, 0,2, 0,4, osv. Dette betyder, at 10%, 20% eller 40% af outputenhederne tilfældigt udelukkes fra det anvendte lag.\n",
    "\n",
    "Opret et nyt neuralt netværk med `tf.keras.layers.Dropout` inden træning med de augmenterede billeder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes, name=\"outputs\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompiler og træn modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisér træningsresultater\n",
    "\n",
    "Efter at have anvendt dataaugmentering og `tf.keras.layers.Dropout` er der mindre overfitting end før, og trænings- og valideringsnøjagtighed er tættere på hinanden:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forudsige på ny data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brug din model til at klassificere et billede, der ikke var inkluderet i trænings- eller valideringssættene.\n",
    "Bemærk: Dataaugmentering og dropout-lag er inaktive under inferenstid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
    "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
    "\n",
    "img = tf.keras.utils.load_img(\n",
    "    sunflower_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brug TensorFlow Lite\n",
    "\n",
    "TensorFlow Lite er et sæt af værktøjer, der muliggør maskinlæring på enheden ved at hjælpe udviklere med at køre deres modeller på mobile, indlejrede og kant-enheder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konverter den Keras Sequential-model til en TensorFlow Lite-model\n",
    "\n",
    "For at bruge den trænede model med on-device-applikationer skal du først [konvertere den](https://www.tensorflow.org/lite/models/convert) til en mindre og mere effektiv modelformat kaldet en [TensorFlow Lite](https://www.tensorflow.org/lite/) model.\n",
    "\n",
    "I dette eksempel tager du den trænede Keras Sequential-model og bruger `tf.lite.TFLiteConverter.from_keras_model` til at generere en [TensorFlow Lite](https://www.tensorflow.org/lite/) model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den TensorFlow Lite-model, du gemte i det forrige trin, kan indeholde flere funktionsunderskrifter. Keras modelkonverterings-API'en bruger automatisk den standardmæssige underskrift. Læs mere om [TensorFlow Lite-underskrifter](https://www.tensorflow.org/lite/guide/signatures).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kør TensorFlow Lite-modellen\n",
    "\n",
    "Du kan få adgang til TensorFlow Lite gemte modelunderskrifter i Python via klassen `tf.lite.Interpreter`.\n",
    "\n",
    "Indlæs modellen med `Interpreter`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_MODEL_FILE_PATH = 'model.tflite' # The default path to the saved TensorFlow Lite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=TF_MODEL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udskriv underskrifterne fra den konverterede model for at få navnene på input (og output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_signature_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dette eksempel har du en standardunderskrift kaldet `serving_default`. Derudover er navnet på 'inputs' `'sequential_1_input'`, mens 'outputs' kaldes `'outputs'`. Du kan finde disse første og sidste Keras-lagnavne ved at køre `Model.summary`, som blev demonstreret tidligere i denne vejledning.\n",
    "\n",
    "Nu kan du teste den indlæste TensorFlow-model ved at udføre inferens på et eksempelbillede med `tf.lite.Interpreter.get_signature_runner` ved at sende underskriftnavnet som følger:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_lite = interpreter.get_signature_runner('serving_default')\n",
    "classify_lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "På samme måde som du gjorde tidligere i vejledningen, kan du bruge TensorFlow Lite-modellen til at klassificere billeder, der ikke var inkluderet i trænings- eller valideringssættene.\n",
    "\n",
    "Du har allerede tensoriseret det billede og gemt det som `img_array`. Nu skal du sende det som det første argument (navnet på 'inputs') til den indlæste TensorFlow Lite-model (`predictions_lite`), beregne softmax-aktivering og derefter udskrive forudsigelsen for klassen med den højeste beregnede sandsynlighed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lite = classify_lite(sequential_1_input=img_array)['outputs']\n",
    "score_lite = tf.nn.softmax(predictions_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score_lite)], 100 * np.max(score_lite))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forudsigelsen genereret af den lette model bør være næsten identisk med forudsigelserne genereret af den originale model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(np.abs(predictions - predictions_lite)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Af de fem klasser - `'daisy'`, `'dandelion'`, `'roses'`, `'sunflowers'` og `'tulips'` - bør modellen forudsige, at billedet hører til solsikker, hvilket er det samme resultat som før konverteringen til TensorFlow Lite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Næste skridt\n",
    "\n",
    "Denne vejledning viste, hvordan man træner en model til billedklassifikation, tester den, konverterer den til TensorFlow Lite-formatet til brug i on-device-applikationer (som f.eks. en billedklassifikationsapp) og udfører inferens med TensorFlow Lite-modellen ved hjælp af Python API'en.\n",
    "\n",
    "Du kan lære mere om TensorFlow Lite gennem [vejledninger](https://www.tensorflow.org/lite/tutorials) og [guider](https://www.tensorflow.org/lite/guide).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
