{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "**Teori** - [**Notion Clustering**](https://www.notion.so/mercantec/Machine-Learning-e89a2baf0d414172b13d07465366482e?pvs=4#2c11e4d51b994deda5dcdbcbb5063c2c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering med manuel lighedsmåling\n",
    "\n",
    "I denne del vil du gruppere chokolader i datasættet [Chocolate Bar Ratings](https://www.kaggle.com/rtatman/chocolate-bar-ratings) ved hjælp af k-means clustering-algoritmen med en manuel lighedsmåling. Datasættet indeholder bedømmelser af chokoladebarer sammen med deres kakaoindhold, bønnetype, bøndeoprindelse, producentnavn og producentland. Du vil:\n",
    "\n",
    "* Indlæse og rengøre data.\n",
    "* Behandle data.\n",
    "* Beregne ligheden mellem par af chokolader.\n",
    "* Cluster chokolader ved hjælp af k-means.\n",
    "* Kontrollere Clusteranalysens resultat ved hjælp af kvalitetsmålinger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Indlæs og rens data\n",
    "\n",
    "Kør følgende celle for at indlæse og rense chokoladedatasættet. Du behøver ikke at\n",
    "forstå koden. De første få rækker af datasættet vises. Undersøg\n",
    "funktionerne og deres værdier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as nla\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import re\n",
    "import pdb  # for Python debugger\n",
    "import sys\n",
    "from os.path import join\n",
    "\n",
    "# Set the output display to have one digit for decimal places and limit it to\n",
    "# printing 15 rows.\n",
    "np.set_printoptions(precision=2)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_rows = 15\n",
    "\n",
    "choc_data = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/flavors_of_cacao.csv\", sep=\",\", encoding='latin-1')\n",
    "\n",
    "# We can rename the columns.\n",
    "choc_data.columns = ['maker', 'specific_origin', 'reference_number', 'review_date', 'cocoa_percent', 'maker_location', 'rating', 'bean_type', 'broad_origin']\n",
    "\n",
    "# choc_data.dtypes\n",
    "\n",
    "# Replace empty/null values with \"Blend\"\n",
    "choc_data['bean_type'] = choc_data['bean_type'].fillna('Blend')\n",
    "\n",
    "#@title Cast bean_type to string to remove leading 'u'\n",
    "choc_data['bean_type'] = choc_data['bean_type'].astype(str)\n",
    "choc_data['cocoa_percent'] = choc_data['cocoa_percent'].str.strip('%')\n",
    "choc_data['cocoa_percent'] = pd.to_numeric(choc_data['cocoa_percent'])\n",
    "\n",
    "#@title Correct spelling mistakes, and replace city with country name\n",
    "choc_data['maker_location'] = choc_data['maker_location']\\\n",
    ".str.replace('Amsterdam', 'Holland')\\\n",
    ".str.replace('U.K.', 'England')\\\n",
    ".str.replace('Niacragua', 'Nicaragua')\\\n",
    ".str.replace('Domincan Republic', 'Dominican Republic')\n",
    "\n",
    "# Adding this so that Holland and Netherlands map to the same country.\n",
    "choc_data['maker_location'] = choc_data['maker_location']\\\n",
    ".str.replace('Holland', 'Netherlands')\n",
    "\n",
    "def cleanup_spelling_abbrev(text):\n",
    "    replacements = [\n",
    "        ['-', ', '], ['/ ', ', '], ['/', ', '], ['\\(', ', '], [' and', ', '], [' &', ', '], ['\\)', ''],\n",
    "        ['Dom Rep|DR|Domin Rep|Dominican Rep,|Domincan Republic', 'Dominican Republic'],\n",
    "        ['Mad,|Mad$', 'Madagascar, '],\n",
    "        ['PNG', 'Papua New Guinea, '],\n",
    "        ['Guat,|Guat$', 'Guatemala, '],\n",
    "        ['Ven,|Ven$|Venez,|Venez$', 'Venezuela, '],\n",
    "        ['Ecu,|Ecu$|Ecuad,|Ecuad$', 'Ecuador, '],\n",
    "        ['Nic,|Nic$', 'Nicaragua, '],\n",
    "        ['Cost Rica', 'Costa Rica'],\n",
    "        ['Mex,|Mex$', 'Mexico, '],\n",
    "        ['Jam,|Jam$', 'Jamaica, '],\n",
    "        ['Haw,|Haw$', 'Hawaii, '],\n",
    "        ['Gre,|Gre$', 'Grenada, '],\n",
    "        ['Tri,|Tri$', 'Trinidad, '],\n",
    "        ['C Am', 'Central America'],\n",
    "        ['S America', 'South America'],\n",
    "        [', $', ''], [',  ', ', '], [', ,', ', '], ['\\xa0', ' '],[',\\s+', ','],\n",
    "        [' Bali', ',Bali']\n",
    "    ]\n",
    "    for i, j in replacements:\n",
    "        text = re.sub(i, j, text)\n",
    "    return text\n",
    "\n",
    "choc_data['specific_origin'] = choc_data['specific_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "#@title Cast specific_origin to string\n",
    "choc_data['specific_origin'] = choc_data['specific_origin'].astype(str)\n",
    "\n",
    "#@title Replace null-valued fields with the same value as for specific_origin\n",
    "choc_data['broad_origin'] = choc_data['broad_origin'].fillna(choc_data['specific_origin'])\n",
    "\n",
    "#@title Clean up spelling mistakes and deal with abbreviations\n",
    "choc_data['broad_origin'] = choc_data['broad_origin'].str.replace('.', '').apply(cleanup_spelling_abbrev)\n",
    "\n",
    "# Change 'Trinitario, Criollo' to \"Criollo, Trinitario\"\n",
    "# Check with choc_data['bean_type'].unique()\n",
    "choc_data.loc[choc_data['bean_type'].isin(['Trinitario, Criollo']),'bean_type'] = \"Criollo, Trinitario\"\n",
    "# Confirm with choc_data[choc_data['bean_type'].isin(['Trinitario, Criollo'])]\n",
    "\n",
    "# Fix chocolate maker names\n",
    "choc_data.loc[choc_data['maker']=='Shattel','maker'] = 'Shattell'\n",
    "choc_data['maker'] = choc_data['maker'].str.replace(u'Na\\xef\\xbf\\xbdve','Naive')\n",
    "\n",
    "# Save the original column names\n",
    "original_cols = choc_data.columns.values\n",
    "\n",
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Forbehandling af data\n",
    "Du vil forbehandle dine data ved hjælp af de teknikker, der beskrives i\n",
    "[Forbered Data](https://developers.google.com/machine-learning/clustering/prepare-data).\n",
    "\n",
    "Lad os starte med trækken `review_date`. Hvis du antager, at fremstillingen af chokolade\n",
    "ikke ændrede sig i løbet af de 10 års data, har `review_date` ingen korrelation\n",
    "med chokoladen selv. Du kan trygt ignorere denne egenskab. Dog, som en god datavidenskabsmand, bør du være nysgerrig om dine data. Lad os\n",
    "plotte fordelingen for `review date` ved hjælp af en funktion fra Seaborn-datavisualiseringsbiblioteket. Det ser ud til, at ingen spiste chokolade i 2009 og 2013. Dog er den samlede tendens for chokoladespisning positiv og meget opmuntrende. Dette er en god\n",
    "tid til at nyde lidt chokolade selv!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(choc_data['review_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plottet for fordelingen af `rating`. Overvej, hvordan du ville behandle denne fordeling. Fortsæt derefter for svaret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution\n",
    "sns.distplot(choc_data['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fordelingen af `rating` minder groft om en Gaussisk fordeling. Hvordan behandles Gaussiske fordelinger? Du ved det. Normaliser dataen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its a Gaussian! So, use z-score to normalize the data\n",
    "choc_data['rating_norm'] = (choc_data['rating'] - choc_data['rating'].mean()\n",
    "                           ) / choc_data['rating'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersøg fordelingen af `cocoa_percent` og overvej, hvordan den skal behandles. Tjek derefter nedenfor for svaret.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(choc_data['cocoa_percent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fordelingen af `cocoa_percent` er tæt nok på en Gaussisk fordeling. Normaliser dataen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choc_data['cocoa_percent_norm'] = (\n",
    "    choc_data['cocoa_percent'] -\n",
    "    choc_data['cocoa_percent'].mean()) / choc_data['cocoa_percent'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vis de første rækker for at kontrollere normaliseringen af `rating` og `cocoa_percent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Du har kakaobønnernes oprindelsesland i `broad_origin` og chokoladens produktionsland i `maker_location`. Men for at beregne ligheden har du brug for landenes længde- og breddegrader. Heldigvis er denne geografiske information tilgængelig i en anden tabel på\n",
    "developers.google.com! Følgende kode downloader Dataset Publishing Language (DSPL)\n",
    "Countries-tabel og sammenkæder den med vores tabel med chokoladeanmeldelser ved hjælp af landets\n",
    "navn som nøgle. Bemærk, at du tilnærmer lande ved længde- og breddegraderne for deres centre.\n",
    "\n",
    "Vis de første rækker for at kontrollere\n",
    "det forarbejdede data. Bemærk de nyoprettede felter `maker_lat`, `maker_long`, `origin_lat` og `origin_long`. Stemmer værdierne i felterne overens med dine forventninger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run code to add latitude and longitude data\n",
    "# Load lat long data\n",
    "\n",
    "countries_info = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/countries_lat_long.csv\", sep=\",\", encoding='latin-1')\n",
    "\n",
    "#Join the chocolate review and geographic information tables on maker country name\n",
    "choc_data = pd.merge(\n",
    "    choc_data, countries_info, left_on=\"maker_location\", right_on=\"name\")\n",
    "choc_data.rename(\n",
    "    columns={\n",
    "        \"longitude\": \"maker_long\",\n",
    "        \"latitude\": \"maker_lat\"\n",
    "    }, inplace=True)\n",
    "choc_data.drop(\n",
    "    columns=[\"name\", \"country\"], inplace=True)  # don't need this data\n",
    "\n",
    "#Join the chocolate review and geographic information tables on origin country name\n",
    "choc_data = pd.merge(\n",
    "    choc_data, countries_info, left_on=\"broad_origin\", right_on=\"name\")\n",
    "choc_data.rename(\n",
    "    columns={\n",
    "        \"longitude\": \"origin_long\",\n",
    "        \"latitude\": \"origin_lat\"\n",
    "    },\n",
    "    inplace=True)\n",
    "choc_data.drop(\n",
    "    columns=[\"name\", \"country\"], inplace=True)  # don't need this data\n",
    "\n",
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kontroller fordelingen af bredde- og længdegrader og overvej, hvordan fordelingerne skal behandles. Tjek derefter nedenfor for svaret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(choc_data['maker_lat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bredde- og længdegrader ikke følger en bestemt fordeling, konverter latitude- og longitude-informationen til kvantiler. Vis de sidste rækker for at bekræfte kvantilværdierne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numQuantiles = 20\n",
    "colsQuantiles = ['maker_lat', 'maker_long', 'origin_lat', 'origin_long']\n",
    "\n",
    "def createQuantiles(dfColumn, numQuantiles):\n",
    "  return pd.qcut(dfColumn, numQuantiles, labels=False, duplicates='drop')\n",
    "\n",
    "\n",
    "for string in colsQuantiles:\n",
    "  choc_data[string] = createQuantiles(choc_data[string], numQuantiles)\n",
    "\n",
    "choc_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kvantilværdierne spænder op til 20. Før kvantilværdierne til den samme skala som andre trækekendetegn ved at skalere dem til [0,1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxScaler(numArr):\n",
    "  minx = np.min(numArr)\n",
    "  maxx = np.max(numArr)\n",
    "  numArr = (numArr - minx) / (maxx - minx)\n",
    "  return numArr\n",
    "\n",
    "\n",
    "for string in colsQuantiles:\n",
    "  choc_data[string] = minMaxScaler(choc_data[string])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trækkene `maker` og `bean_type` er kategoriske træk. Konverter\n",
    "kategoriske træk til one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate the \"maker\" feature since it's removed by one-hot encoding function\n",
    "choc_data['maker2'] = choc_data['maker']\n",
    "choc_data = pd.get_dummies(choc_data, columns=['maker2'], prefix=['maker'])\n",
    "# similarly, duplicate the \"bean_type\" feature\n",
    "choc_data['bean_type2'] = choc_data['bean_type']\n",
    "choc_data = pd.get_dummies(choc_data, columns=['bean_type2'], prefix=['bean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efter clustering, når du fortolker resultaterne, kan det forarbejdede trækdata være\n",
    "svært at læse. Gem de oprindelige trækdata i en ny dataramme, så du kan\n",
    "referere til dem senere. Behold kun de forarbejdede data i `choc_data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataframe into two frames: Original data and data for clustering\n",
    "choc_data_backup = choc_data.loc[:, original_cols].copy(deep=True)\n",
    "choc_data.drop(columns=original_cols, inplace=True)\n",
    "\n",
    "# get_dummies returned ints for one-hot encoding but we want floats so divide by\n",
    "# 1.0\n",
    "# Note: In the latest version of \"get_dummies\", you can set \"dtype\" to float\n",
    "choc_data = choc_data / 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gennemgå de sidste poster for at sikre, at dine dyrebare chokoladedata ser\n",
    "godt ud! Husk, at `choc_data` kun viser kolonner med forarbejdede data, da de kolonner, der indeholder de originale data, blev flyttet til `choc_data_backup`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choc_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Beregn Manuel Lighed\n",
    "Du har arbejdet hårdt på at forarbejde dataene! Nu er det enkelt at beregne ligheden mellem et\n",
    "par chokolader, fordi alle træk er numeriske og i\n",
    "samme område. For to chokolader skal du blot finde den kvadratrods middelkvadratfejl\n",
    "(RMSE) for alle træk.\n",
    "\n",
    "Kør først denne kode for at definere lighedsfunktionen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(obj1, obj2):\n",
    "  len1 = len(obj1.index)\n",
    "  len2 = len(obj2.index)\n",
    "  if not (len1 == len2):\n",
    "    print \"Error: Compared objects must have same number of features.\"\n",
    "    sys.exit()\n",
    "    return 0\n",
    "  else:\n",
    "    similarity = obj1 - obj2\n",
    "    similarity = np.sum((similarity**2.0) / 10.0)\n",
    "    similarity = 1 - math.sqrt(similarity)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregn nu ligheden mellem den første chokolade og de næste 4\n",
    "chokolader. Bekræft den beregnede lighed i forhold til dine intuitive forventninger\n",
    "ved at sammenligne den beregnede lighed med de faktiske trækdata, der vises i\n",
    "næste celle.\n",
    "\n",
    "Hvis du er nysgerrig efter ligheder mellem andre chokolader, kan du ændre\n",
    "koden nedenfor og tage et kig!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choc1 = 0  #@param\n",
    "chocsToCompare = [1, 4]  #@param\n",
    "\n",
    "print \"Similarity between chocolates \" + str(choc1) + \" and ...\"\n",
    "\n",
    "for ii in range(chocsToCompare[0], chocsToCompare[1] + 1):\n",
    "  print str(ii) + \": \" + str(\n",
    "      getSimilarity(choc_data.loc[choc1], choc_data.loc[ii]))\n",
    "\n",
    "print \"\\n\\nFeature data for chocolate \" + str(choc1)\n",
    "print choc_data_backup.loc[choc1:choc1, :]\n",
    "print \"\\n\\nFeature data for compared chocolates \" + str(chocsToCompare)\n",
    "print choc_data_backup.loc[chocsToCompare[0]:chocsToCompare[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Klynge Chokoladedataset\n",
    "\n",
    "Vi er klar til at klynge chokoladerne! Kør koden for at opsætte k-means\n",
    "klyngefunktionerne. Du behøver ikke at forstå koden.\n",
    "\n",
    "**Bemærk**: Hvis du følger selvstudiet, skal du inden du kører resten af\n",
    "dette Colab-læringsmiljø læse afsnittene om\n",
    "[k-means](https://developers.google.com/machine-learning/clustering/algorithm/run-algorithm)\n",
    "og\n",
    "[kvalitetsmålinger](https://developers.google.com/machine-learning/clustering/interpret).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run cell to setup functions\n",
    "def dfSimilarity(df, centroids):\n",
    "  ### dfSimilarity = Calculate similarities for dataframe input\n",
    "  ### We need to calculate ||a-b||^2 = |a|^2 + |b|^2 - 2*|a|*|b|\n",
    "  ### Implement this with matrix operations\n",
    "  ### See the Appendix for further explanation\n",
    "  numPoints = len(df.index)\n",
    "  numCentroids = len(centroids.index)\n",
    "  ## Strictly speaking, we don't need to calculate the norm of points\n",
    "  # because it adds a constant bias to distances\n",
    "  # But calculating it so that the similarity doesn't go negative\n",
    "  # And that we expect similarities in [0,1] which aids debugging\n",
    "  pointNorms = np.square(nla.norm(df, axis=1))\n",
    "  pointNorms = np.reshape(pointNorms, [numPoints, 1])\n",
    "  ## Calculate the norm of centroids\n",
    "  centroidNorms = np.square(nla.norm(centroids, axis=1))\n",
    "  centroidNorms = np.reshape(centroidNorms, (1, numCentroids))\n",
    "  ## Calculate |a|^2 + |b|^2 - 2*|a|*|b|\n",
    "  similarities = pointNorms + centroidNorms - 2.0 * np.dot(\n",
    "      df, np.transpose(centroids))\n",
    "  # Divide by the number of features\n",
    "  # Which is 10 because the one-hot encoding means the \"Maker\" and \"Bean\" are\n",
    "  # weighted twice\n",
    "  similarities = similarities / 10.0\n",
    "  # numerical artifacts lead to negligible but negative values that go to NaN on the root\n",
    "  similarities = similarities.clip(min=0.0)\n",
    "  # Square root since it's ||a-b||^2\n",
    "  similarities = np.sqrt(similarities)\n",
    "  return similarities\n",
    "\n",
    "\n",
    "def initCentroids(df, k, feature_cols):\n",
    "  # Pick 'k' examples are random to serve as initial centroids\n",
    "  limit = len(df.index)\n",
    "  centroids_key = np.random.randint(0, limit - 1, k)\n",
    "  centroids = df.loc[centroids_key, feature_cols].copy(deep=True)\n",
    "  # the indexes get copied over so reset them\n",
    "  centroids.reset_index(drop=True, inplace=True)\n",
    "  return centroids\n",
    "\n",
    "\n",
    "def pt2centroid(df, centroids, feature_cols):\n",
    "  ### Calculate similarities between all points and centroids\n",
    "  ### And assign points to the closest centroid + save that distance\n",
    "  numCentroids = len(centroids.index)\n",
    "  numExamples = len(df.index)\n",
    "  # dfSimilarity = Calculate similarities for dataframe input\n",
    "  dist = dfSimilarity(df.loc[:, feature_cols], centroids.loc[:, feature_cols])\n",
    "  df.loc[:, 'centroid'] = np.argmin(dist, axis=1)  # closest centroid\n",
    "  df.loc[:, 'pt2centroid'] = np.min(dist, axis=1)  # minimum distance\n",
    "  return df\n",
    "\n",
    "\n",
    "def recomputeCentroids(df, centroids, feature_cols):\n",
    "  ### For every centroid, recompute it as an average of the points\n",
    "  ### assigned to it\n",
    "  numCentroids = len(centroids.index)\n",
    "  for cen in range(numCentroids):\n",
    "    dfSubset = df.loc[df['centroid'] == cen,\n",
    "                      feature_cols]  # all points for centroid\n",
    "    if not (dfSubset.empty):  # if there are points assigned to the centroid\n",
    "      clusterAvg = np.sum(dfSubset) / len(dfSubset.index)\n",
    "      centroids.loc[cen] = clusterAvg\n",
    "  return centroids\n",
    "\n",
    "\n",
    "def kmeans(df, k, feature_cols, verbose):\n",
    "  flagConvergence = False\n",
    "  maxIter = 100\n",
    "  iter = 0  # ensure kmeans doesn't run for ever\n",
    "  centroids = initCentroids(df, k, feature_cols)\n",
    "  while not (flagConvergence):\n",
    "    iter += 1\n",
    "    #Save old mapping of points to centroids\n",
    "    oldMapping = df['centroid'].copy(deep=True)\n",
    "    # Perform k-means\n",
    "    df = pt2centroid(df, centroids, feature_cols)\n",
    "    centroids = recomputeCentroids(df, centroids, feature_cols)\n",
    "    # Check convergence by comparing [oldMapping, newMapping]\n",
    "    newMapping = df['centroid']\n",
    "    flagConvergence = all(oldMapping == newMapping)\n",
    "    if verbose == 1:\n",
    "      print 'Total distance:' + str(np.sum(df['pt2centroid']))\n",
    "    if (iter > maxIter):\n",
    "      print 'k-means did not converge! Reached maximum iteration limit of ' \\\n",
    "            + str(maxIter) + '.'\n",
    "      sys.exit()\n",
    "      return\n",
    "  print 'k-means converged for ' + str(k) + ' clusters' + \\\n",
    "        ' after ' + str(iter) + ' iterations!'\n",
    "  return [df, centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kør cellen for at klynge chokoladedatasættet, hvor `k` er antallet af\n",
    "klynger.\n",
    "\n",
    "På hver iteration af k-means viser output, hvordan summen af afstande fra alle eksempler til deres centroider reduceres, således at k-means altid konvergerer. Den følgende tabel viser data for de første chokolader. Længst til højre i tabellen skal du kontrollere den tildelte centroid for hvert eksempel i kolonnen `centroid` og afstanden fra eksemplet til dens centroid i kolonnen `pt2centroid`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30  #@param\n",
    "\n",
    "feature_cols = choc_data.columns.values  # save original columns\n",
    "# initialize every point to an impossible value, the k+1 cluster\n",
    "choc_data['centroid'] = k\n",
    "# init the point to centroid distance to an impossible value \"2\" (>1)\n",
    "choc_data['pt2centroid'] = 2\n",
    "[choc_data, centroids] = kmeans(choc_data, k, feature_cols, 1)\n",
    "print(\"Data for the first few chocolates, with 'centroid' and 'pt2centroid' on\"\n",
    "      ' the extreme right:')\n",
    "choc_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gennemse Klynge Resultat\n",
    "Gennemse chokoladerne i forskellige klynger ved at ændre parametret `clusterNumber`\n",
    "i den næste celle og køre cellen. Overvej disse spørgsmål, mens du undersøger klyngerne:\n",
    "\n",
    "*   Er klyngerne meningsfulde?\n",
    "*   Vejer klyngerne visse træk mere end andre? Hvorfor?\n",
    "*   Gør ændring af antallet af klynger klyngerne mere eller mindre\n",
    "    meningsfulde?\n",
    "\n",
    "Efter at have overvejet disse spørgsmål, kan du udvide næste afsnit for en diskussion af klynge-resultaterne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNumber = 7  #@param\n",
    "choc_data_backup.loc[choc_data['centroid'] == clusterNumber, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Løsning: Diskussion af Clustering Resultater\n",
    "\n",
    "Klik nedenfor for svaret.\n",
    "**Diskussion**: Klynge-resultatet vægter visse træk mere end andre uden hensigt.\n",
    "\n",
    "Dette skyldes, at en given chokoladeproducent vil have samme produktionsland, hvilket fører til gensidig information mellem trækkene `maker`,\n",
    "`maker_lat` og `maker_long`. På samme måde, hvis hvert land har tendens til at dyrke en\n",
    "bestemt type bønne, er der gensidig information mellem `origin_lat`,\n",
    "`origin_long` og `bean_type`.\n",
    "\n",
    "Som følge heraf vægter træk, der deler gensidig information, effektivt mere end uafhængige træk. Løsningen er at bruge en overvåget lighedsmåling, fordi DNN eliminerer korreleret information. Se\n",
    "[k-means fordele og ulemper](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).\n",
    "\n",
    "Nu skal du overveje one-hot encoding. Chokolader, der er lavet af forskellige producenter, vil\n",
    "afvige med 1 i to kolonner. På samme måde vil chokolader, der er lavet af forskellige\n",
    "bønnetyper, afvige med 1 i to træk. Derfor vil forskelle i producenter og bønnetyper blive vægtet dobbelt så meget som andre træk. Denne skæve vægtning fordrejer klynge-resultatet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Kvalitetsmetrikker for Klynger\n",
    "\n",
    "For klyngerne skal vi beregne de metrikker, der er diskuteret i\n",
    "[Fortolk Resultater](https://developers.google.com/machine-learning/clustering/interpret).\n",
    "Læs det kursusindhold, inden du begynder på dette kodeafsnit.\n",
    "\n",
    "Kør den næste celle for at opsætte funktioner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run cell to set up functions { display-mode: \"form\" }\n",
    "def clusterCardinality(df):\n",
    "  k = np.max(df['centroid']) + 1\n",
    "  k = k.astype(int)\n",
    "  print 'Number of clusters:' + str(k)\n",
    "  clCard = np.zeros(k)\n",
    "  for kk in range(k):\n",
    "    clCard[kk] = np.sum(df['centroid'] == kk)\n",
    "  clCard = clCard.astype(int)\n",
    "  # print \"Cluster Cardinality:\"+str(clCard)\n",
    "  plt.figure()\n",
    "  plt.bar(range(k), clCard)\n",
    "  plt.title('Cluster Cardinality')\n",
    "  plt.xlabel('Cluster Number: ' + str(0) + ' to ' + str(k - 1))\n",
    "  plt.ylabel('Points in Cluster')\n",
    "  return clCard\n",
    "\n",
    "\n",
    "def clusterMagnitude(df):\n",
    "  k = np.max(df['centroid']) + 1\n",
    "  k = k.astype(int)\n",
    "  cl = np.zeros(k)\n",
    "  clMag = np.zeros(k)\n",
    "  for kk in range(k):\n",
    "    idx = np.where(df['centroid'] == kk)\n",
    "    idx = idx[0]\n",
    "    clMag[kk] = np.sum(df.loc[idx, 'pt2centroid'])\n",
    "  # print \"Cluster Magnitude:\",clMag #precision set using np pref\n",
    "  plt.figure()\n",
    "  plt.bar(range(k), clMag)\n",
    "  plt.title('Cluster Magnitude')\n",
    "  plt.xlabel('Cluster Number: ' + str(0) + ' to ' + str(k - 1))\n",
    "  plt.ylabel('Total Point-to-Centroid Distance')\n",
    "  return clMag\n",
    "\n",
    "\n",
    "def plotCardVsMag(clCard, clMag):\n",
    "  plt.figure()\n",
    "  plt.scatter(clCard, clMag)\n",
    "  plt.xlim(xmin=0)\n",
    "  plt.ylim(ymin=0)\n",
    "  plt.title('Magnitude vs Cardinality')\n",
    "  plt.ylabel('Magnitude')\n",
    "  plt.xlabel('Cardinality')\n",
    "\n",
    "\n",
    "def clusterQualityMetrics(df):\n",
    "  clCard = clusterCardinality(df)\n",
    "  clMag = clusterMagnitude(df)\n",
    "  plotCardVsMag(clCard, clMag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beregn følgende metrikker ved at køre den næste celle:\n",
    "\n",
    "*   kardinaliteten af dine klynger\n",
    "*   størrelsen af dine klynger\n",
    "*   kardinalitet kontra størrelse\n",
    "\n",
    "Fra plot skal du finde klynger, der er outliers, og klynger, der er gennemsnitlige.\n",
    "Sammenlign eksempler i outlier-klynger med dem i gennemsnitlige klynger ved at ændre `clusterNumber` i det foregående afsnit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterQualityMetrics(choc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find det optimale antal klynger\n",
    "\n",
    "Du vil finde det rigtige antal klynger, som du gjorde i den tidligere\n",
    "programmeringsøvelse. For detaljer, læs \"*Trin tre: Optimalt antal\n",
    "klynger*\" på siden\n",
    "[Fortolk Resultater](https://developers.google.com/machine-learning/clustering/interpret).\n",
    "\n",
    "Kør koden nedenfor. Følger plottet formen vist på \"*Fortolk Resultater*\"? Hvad er det\n",
    "optimale antal klynger? Eksperimenter med parametrene nedenfor, hvis det er nødvendigt. Efter at have overvejet spørgsmålene, kan du udvide næste afsnit for en diskussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Løsning: Diskussion om det optimale antal klynger\n",
    "\n",
    "Klik nedenfor for løsningen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss vs number of clusters\n",
    "def lossVsClusters(kmin, kmax, kstep, choc_data):\n",
    "  kmax += 1  # include kmax-th cluster in range\n",
    "  kRange = range(kmin, kmax, kstep)\n",
    "  loss = np.zeros(len(kRange))\n",
    "  lossCtr = 0\n",
    "  for kk in kRange:\n",
    "    [choc_data, centroids] = kmeans(choc_data, kk, feature_cols, 0)\n",
    "    loss[lossCtr] = np.sum(choc_data['pt2centroid'])\n",
    "    lossCtr += 1\n",
    "  plt.scatter(kRange, loss)\n",
    "  plt.title('Loss vs Clusters Used')\n",
    "  plt.xlabel('Number of clusters')\n",
    "  plt.ylabel('Total Point-to-Centroid Distance')\n",
    "\n",
    "\n",
    "kmin = 5  # @param\n",
    "kmax = 80  # @param\n",
    "kstep = 2  # @param\n",
    "lossVsClusters(kmin, kmax, kstep, choc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diskussion**: Den ideelle graf over tab versus antal klynger har et tydeligt vendepunkt, ud over hvilket tabets fald flader ud. Her mangler grafen et tydeligt vendepunkt. Dog flader tabet ud to gange, ved cirka `k = 15`\n",
    "og `k = 35`, hvilket antyder, at `k` har optimale værdier tæt på 15 og 35. Bemærk, at din graf kan variere på grund af den iboende tilfældighed i k-means algoritmen.\n",
    "\n",
    "Du\n",
    "ser typisk en graf med et tydeligt vendepunkt, når data naturligt danner klumper.\n",
    "Når data ikke har naturlige klumper, giver denne graf kun en antydning\n",
    "om den optimale værdi for `k`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diskussion\n",
    "\n",
    "På siden\n",
    "[Superviseret Lighedsmåling](https://developers.google.com/machine-learning/clustering/similarity/supervised-similarity),\n",
    "læs \"*Comparison of Manual and Supervised Measures*\". Forsøg at forbinde beskrivelsen af en manuel lighedsmåling med det, du har lært i denne kodeøvelse. Klik derefter nedenfor for at se diskussionen. Til sidst, **hold denne Colab åben** for at sammenligne resultaterne med den næste Colab, der bruger en superviseret lighedsmåling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab'en demonstrerer følgende karakteristika ved en manuel lighedsmåling:\n",
    "\n",
    "*   **Eliminerer ikke overflødig information i korrelerede træk**. Som\n",
    "    diskuteret i dette [afsnit](#scrollTo=MJtuP9w5jJHq), eliminerede vores manuelle lighedsmåling ikke overflødig information mellem træk.\n",
    "*   **Giver indsigt i beregnede ligheder**. Ved at se på klynge-resultaterne kunne du se, hvordan producentens placering og bønnens oprindelse havde en større indflydelse på klynge-resultatet. Du så, hvordan one-hot encoding\n",
    "    resulterede i, at producent og bønnetype blev vægtet dobbelt så meget som andre træk.\n",
    "*   **Egnet til små datasæt med få træk**. Ja, du kunne nemt\n",
    "    konstruere en manuel lighedsmåling for chokoladedatasættet, da det har\n",
    "    færre end to tusind eksempler og kun ni træk.\n",
    "*   **Ikke egnet til store datasæt med mange træk**. Hvis chokoladedatasættet\n",
    "    havde dusinvis af træk og mange tusind eksempler, ville det være vanskeligt\n",
    "    at konstruere en korrekt lighedsmåling og derefter verificere lighedsmålingen\n",
    "    på tværs af datasættet.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
